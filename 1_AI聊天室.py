# pages/1_ğŸ¤–_AI_èŠå¤©å®¤.py
import streamlit as st
import google.generativeai as genai

# --- Gemini API Configuration ---
st.sidebar.subheader("ğŸ”‘ Gemini API è¨­å®š")
api_key = st.sidebar.text_input("è¼¸å…¥ Gemini API Key", type="password")

if api_key:
    st.session_state["GOOGLE_API_KEY"] = api_key

if "GOOGLE_API_KEY" not in st.session_state:
    st.error("âŒ è«‹å…ˆåœ¨å´é‚Šæ¬„è¼¸å…¥ Gemini API Key")
    st.stop()
else:
    genai.configure(api_key=st.session_state["GOOGLE_API_KEY"])

# --- åˆå§‹åŒ– Gemini æ¨¡å‹ ---
@st.cache_resource(ttl="30min")
def get_gemini_model():
    return genai.GenerativeModel("gemini-2.5-flash")

gemini_model = get_gemini_model()

# --- åˆå§‹åŒ–èŠå¤© ---
if "chat" not in st.session_state:
    st.session_state.chat = gemini_model.start_chat(history=[])

st.title("ğŸ’¬ AI è²¡å‹™èŠå¤©æ©Ÿå™¨äºº")
st.markdown("---")
st.write("æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ AI è²¡å‹™åˆ†æåŠ©ç†ã€‚")
st.write("æ‚¨å¯ä»¥å•æˆ‘ä»»ä½•é—œæ–¼è²¡å‹™åˆ†æã€å¸‚å ´è¶¨å‹¢æˆ–æ‚¨ä¸Šå‚³æ•¸æ“šä¸­å…¬å¸çš„å•é¡Œã€‚")

# --- é¡¯ç¤ºæ­·å²è¨Šæ¯ ---
for message in st.session_state.chat.history:
    role = "user" if message.role == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(message.parts[0].text)

# --- èŠå¤©è¼¸å…¥ ---
user_query = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...")

if user_query:
    # é¡¯ç¤ºä½¿ç”¨è€…è¨Šæ¯
    with st.chat_message("user"):
        st.markdown(user_query)

    # åŠ ä¸Š CSV æ•¸æ“šæ‘˜è¦ (å¦‚æœæœ‰ä¸Šå‚³)
    processed_df_str = ""
    if "processed_df" in st.session_state:
        df_summary = st.session_state["processed_df"].head().to_markdown(index=False)
        processed_df_str = f"\n\nä»¥ä¸‹æ˜¯æ‚¨ä¸Šå‚³çš„æ•¸æ“šæ‘˜è¦ (å‰ 5 ç­†)ï¼š\n{df_summary}\n\n"

    # çµ„åˆå®Œæ•´ prompt
    full_prompt = f"{user_query}{processed_df_str}\nè«‹ç”¨ä¸­æ–‡å›ç­”ã€‚"

    # AI å›è¦†
    with st.chat_message("assistant"):
        with st.spinner("AI æ€è€ƒä¸­..."):
            try:
                response = st.session_state.chat.send_message(full_prompt)
                if response and response.text:
                    st.markdown(response.text)
                else:
                    st.warning("âš ï¸ AI ç„¡æ³•ç”Ÿæˆå›è¦†ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
            except Exception as e:
                st.error(f"èŠå¤©æ©Ÿå™¨äººéŒ¯èª¤: {e}")
