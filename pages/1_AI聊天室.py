import streamlit as st
import google.generativeai as genai
import pandas as pd
import os

# --- Gemini API Configuration ---
# ç¢ºä¿ API key å·²é…ç½®
if "GOOGLE_API_KEY" not in st.secrets:
    st.error("Gemini API key not found in .streamlit/secrets.toml. è«‹ç¢ºä¿æ‚¨çš„ä¸»æ‡‰ç”¨ç¨‹å¼æª”æ¡ˆå·²é…ç½® API keyã€‚")
    st.stop()
genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])

# åˆå§‹åŒ– Gemini Pro æ¨¡å‹å’ŒèŠå¤©æœƒè©±
# ä½¿ç”¨ st.cache_resource ä¾†å¿«å–æ¨¡å‹ï¼Œé¿å…æ¯æ¬¡é é¢é‡æ–°é‹è¡Œæ™‚éƒ½é‡æ–°åˆå§‹åŒ–
@st.cache_resource(ttl="30min")
def get_gemini_model():
    try:
        return genai.GenerativeModel('gemini-2.5-flash')
    except Exception as e:
        st.error(f"æœªèƒ½åˆå§‹åŒ– Gemini æ¨¡å‹: {e}")
        st.stop()

gemini_model = get_gemini_model()

# åˆå§‹åŒ–èŠå¤©æœƒè©±ï¼Œä¸¦å°‡æ­·å²è¨˜éŒ„å„²å­˜åœ¨ session_state ä¸­
# é€™è£¡ä¿®æ”¹äº†åˆå§‹åŒ–çš„ historyï¼Œçµ¦äºˆAIä¸€å€‹é€šç”¨çš„è§’è‰²æç¤º
if "chat" not in st.session_state:
    st.session_state.chat = gemini_model.start_chat(history=[
        {"role": "user", "parts": ["ä½ æ˜¯ä¸€å€‹å¤šåŠŸèƒ½AIåŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç¨®ä¸»é¡Œçš„å•é¡Œï¼Œä¸é™æ–¼ç‰¹å®šé ˜åŸŸã€‚"]},
        {"role": "model", "parts": ["å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚æˆ‘æœƒç›¡åŠ›å›ç­”æ‚¨çš„å„ç¨®å•é¡Œã€‚è«‹å•æœ‰ä»€éº¼å¯ä»¥å¹«åŠ©æ‚¨çš„å—ï¼Ÿ"]}
    ])

st.title("ğŸ’¬ AI èŠå¤©æ©Ÿå™¨äºº (é€šç”¨å‹)") # ä¿®æ”¹æ¨™é¡Œï¼Œè®“ç”¨æˆ¶çŸ¥é“é€™æ˜¯é€šç”¨å‹
st.markdown("---")
st.write("æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ AI åŠ©ç†ã€‚æ‚¨å¯ä»¥å•æˆ‘ä»»ä½•å•é¡Œï¼Œä¸è«–æ˜¯è²¡å‹™ç›¸é—œé‚„æ˜¯å…¶ä»–ä¸»é¡Œï¼Œæˆ‘éƒ½æœƒç›¡åŠ›ç‚ºæ‚¨æä¾›å¹«åŠ©ã€‚")
st.write("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼Œæˆ‘æœƒç›¡åŠ›ç‚ºæ‚¨æä¾›å¹«åŠ©ã€‚")

# é¡¯ç¤ºèŠå¤©æ­·å²è¨˜éŒ„
for message in st.session_state.chat.history:
    role = "user" if message.role == "user" else "assistant"
    with st.chat_message(role):
        st.markdown(message.parts[0].text)

# èŠå¤©è¼¸å…¥æ¡†
user_query = st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...")

if user_query:
    # å°‡ç”¨æˆ¶æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©æ­·å²ä¸¦é¡¯ç¤º
    with st.chat_message("user"):
        st.markdown(user_query)
    
    # ç²å– AI å›è¦†
    with st.chat_message("assistant"):
        with st.spinner("AI æ€è€ƒä¸­..."):
            try:
                # æº–å‚™è²¡å‹™æ•¸æ“šä¸Šä¸‹æ–‡ï¼Œä½†ä»¥æ›´ä¸­ç«‹çš„æ–¹å¼å‚³é
                processed_df_context = ""
                if 'processed_df' in st.session_state and not st.session_state['processed_df'].empty:
                    df_summary = st.session_state['processed_df'].head(5).to_markdown(index=False)
                    # é€™è£¡å°‡è²¡å‹™æ•¸æ“šæè¿°ç‚ºã€Œå¯åƒè€ƒçš„è³‡è¨Šã€ï¼Œè€Œä¸æ˜¯ä¸»è¦åˆ†æå°è±¡
                    processed_df_context = f"\n\n[åƒè€ƒè³‡è¨Šï¼šé€™è£¡æœ‰ä¸€ä»½æ‚¨ä¸Šå‚³çš„è²¡å‹™æ•¸æ“šæ‘˜è¦ï¼Œå¦‚æœæˆ‘çš„å•é¡Œèˆ‡è²¡å‹™ç›¸é—œï¼Œè«‹åƒè€ƒé€™äº›æ•¸æ“šé€²è¡Œå›ç­”ï¼š\n{df_summary}\n]\n"
                
                # çµ„åˆæœ€çµ‚å‚³çµ¦ AI çš„æç¤ºè©
                # æˆ‘å€‘å·²ç¶“åœ¨ start_chat æ™‚è¨­å®šäº†é€šç”¨è§’è‰²ï¼Œé€™è£¡åªéœ€å‚³éç”¨æˆ¶å•é¡Œå’Œå¯èƒ½çš„æ•¸æ“šä¸Šä¸‹æ–‡
                full_prompt = f"{user_query}{processed_df_context}\nè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"
                
                response = st.session_state.chat.send_message(full_prompt)
                
                if response and response.text:
                    st.markdown(response.text)
                else:
                    st.warning("AI ç„¡æ³•ç”Ÿæˆå›è¦†ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
            except Exception as e:
                st.error(f"èŠå¤©æ©Ÿå™¨äººéŒ¯èª¤: {e}")